---
title: "HW13"
author: "Zach White"
date: "10/31/2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(MASS)
library(monomvn)
library(glmnet)
```

```{r, cache = TRUE}
n = 50
sigma = 2.5
betatrue = c(4,2,0,0,0,-1,0,1.5, 0,0,0,1,0,.5,0,0,0,0,-1,1,4)
#          int|    X1                            | X2     |X3          
# set the random seed so that we can replicate results.
set.seed(42)

fname=rep("df",100)

# create 100 datasets
for (i in 1:100) {
  Z = matrix(rnorm(n*10, 0, 1), ncol=10, nrow=n)
  X1 = cbind(Z, (Z[,1:5] %*% c(.3, .5, .7, .9, 1.1) %*% t(rep(1,5)) +
             matrix(rnorm(n*5, 0, 1), ncol=5, nrow=n)))
  X2 <- matrix(rnorm(n*4,0,1), ncol=4, nrow=n)
  X3 <- X2[,4]+rnorm(n,0,sd=0.1)
  X <- cbind(X1,X2,X3)
  X = sweep(X, 2, apply(X,2, mean))  # subtract off the column means
  Y = 4 + X %*% betatrue[-1] + rnorm(n,0,sigma)  # does not have a column of ones for the intercept
  df = data.frame(Y, X)
  fname[i] = paste("df", as.character(i), sep="")
  save(df, file=fname[i])
}

```

# Question 1

## Part A
We know that $Y \sim N(X\beta, I_n\sigma^2)$
$$
\begin{aligned}
E[(\hat\beta - \beta)^{T}(\hat\beta - \beta)] &= \sigma^2tr[(X^TX)^-1] \\
&= \sigma^2\sum_{j=1}^{p} \lambda_j^{-1}
\end{aligned}
$$
```{r}
sigma^2 * sum(diag(solve(t(X) %*% X)))
```


## Part B
```{r, cache = TRUE}
MSE.OLS = rep(0,100)

for( i in 1:100) {
  rm(df)
  load(fname[i])
  nk.ols = lm(Y ~ ., data=df)
  coef.ols = coef(nk.ols)
  MSE.OLS[i] = sum((betatrue - coef.ols)^2)
#  print(c(i, MSE.OLS[i]))
}

mean(MSE.OLS)
hist(MSE.OLS, freq = FALSE)
lines(density(MSE.OLS))
```

More simulations might be good because it is very right skewed.

## Part C
```{r, cache = TRUE}

MSE.OLS  = rep(0,100)
MSE.ridge = rep(0,100)
MSE.lasso = rep(0,100)
MSE.horse = rep(0,100)
var.OLS = rep(0,100)
var.ridge = rep(0,100)
var.lasso = rep(0,100)
var.horse = rep(0,100)
## Right here, modify with lasso, ridge, and horseshoe.  Check out standardization
for( i in 1:100) {
  rm(df)
  load(fname[i])
  X = as.matrix(df[,2:dim(df)[2]])
  Y = df$Y
  nk.ols = lm(Y ~ ., data=df)
  coef.ols = coef(nk.ols)
  MSE.OLS[i] = sum((betatrue - coef.ols)^2)
  var.OLS[i] = var(coef.ols)
  
  train=sample(1: nrow(X), nrow(X)/5)
  ## Lasso
  lass.out = cv.glmnet(X[train,],Y[train], alpha = 1)
  best.lam.lass = lass.out$lambda.min
  lasso.mod = glmnet(X,Y,alpha = 1, lambda = best.lam.lass)
  lasso.coefs = coef(lasso.mod)
  MSE.lasso[i] = sum((betatrue - lasso.coefs)^2)
  var.lasso[i] = var(lasso.coefs[,1])
  ## Ridge
  train=sample(1: nrow(X), nrow(X)/5)
  #ridge.mod = glmnet(X,Y,alpha = 0)
  cv.out = cv.glmnet(X[train,],Y[train],alpha = 0)
  best.lam = cv.out$lambda.min
  fin.ridge.mod = glmnet(X,Y,alpha =0,lambda = best.lam)
  ridge.coefs = coef(fin.ridge.mod)
  MSE.ridge[i] = sum((betatrue-ridge.coefs)^2)
  var.ridge[i] = var(ridge.coefs[,1])
  ## Horseshoe
  horse.mod = bhs(X,Y)
  horse.betas = cbind(horse.mod$mu,horse.mod$beta)
  horse.coefs = apply(horse.betas,2,mean)
  MSE.horse[i] = sum((betatrue - horse.coefs)^2)
  var.horse[i] = var(horse.coefs)
}
```

```{r}
par(mfrow = c(2,2))
hist(MSE.OLS, main = "MSE OLS", freq = FALSE)
lines(density(MSE.OLS))
hist(MSE.lasso, main = "MSE LASSO", freq = FALSE)
lines(density(MSE.lasso))
hist(MSE.ridge, main = "MSE Ridge", freq = FALSE)
lines(density(MSE.ridge))
hist(MSE.horse, main = "MSE Horsehoe", freq = FALSE)

MSE = cbind(MSE.OLS,MSE.lasso,MSE.ridge,MSE.horse)
names(MSE) = c("OLS","Lasso","Ridge","Horseshoe")
boxplot(MSE, las = 2)

```
